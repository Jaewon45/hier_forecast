{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c86cf1",
   "metadata": {},
   "source": [
    "# Hierarchical Time Series Forecasting Pipeline\n",
    "\n",
    "This notebook demonstrates our pipeline for comparison of model and reconciliation approaches in time-series forecasting as part of our research/consulting project with BASF. An interactive analysis of the results is provided separately in the notebook title results_analysis.ipynb and deeper discussion is available in our slides or paper. When reading, it may help to keep in mind that throughout this project, EBIT is the target variable and by default lies in the middle of the data's hierarchy.\n",
    "\n",
    "This pipeline includes:\n",
    "- Multiple forecasting models\n",
    "- Multiple hierarchical reconciliation methods\n",
    "- Option for feature engineering and added covariates\n",
    "- Option to switch hierarchical paths as outlined in paper/slides\n",
    "- Standardized model comparison framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945fd69",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5208d54",
   "metadata": {},
   "source": [
    "### Package Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "164e9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may need to install darts depending on environment\n",
    "#!python -m pip install darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd47d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from darts.models import (Prophet, LinearRegressionModel, ARIMA,  ExponentialSmoothing, XGBModel,  NBEATSModel, GlobalNaiveAggregate, NaiveDrift)\n",
    "from darts.dataprocessing.transformers import MinTReconciliator, BottomUpReconciliator, TopDownReconciliator\n",
    "from utils import (get_winners,get_best_per_series, load_data, apply_hierarchy, compare_models_multivariate, compare_models_reconciliated, compare_models_univariate)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19af2b9",
   "metadata": {},
   "source": [
    "Ignore excessive noise by silencing some warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07ffab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcc577",
   "metadata": {},
   "source": [
    "### Hierarchy Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc98ad",
   "metadata": {},
   "source": [
    "Here you can change which subset of the data and corresponding hierarchical structure to use. \n",
    "\n",
    "The options are:\n",
    "- 'var0': the whole hierarchy, includes both other paths\n",
    "- 'var1': EBIT - DA = EBITDA\n",
    "- 'var2': Net Sales  - Variable Costs = CM and CM - Fix Costs = EBIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8a02c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hvar = 'var2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3f60d",
   "metadata": {},
   "source": [
    "### Data Import and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486b088",
   "metadata": {},
   "source": [
    "Import data, apply hierarchy, and split into test-validation sets. Versions of the data both with and without the hierarchy are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ca70821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(file_path='data/SampleHierForecastingBASF_share.xlsx')\n",
    "series, target, covariates, hierarchy = apply_hierarchy(df, hvar=hvar)\n",
    "train, val = target[:-24], target[-24:-12]\n",
    "hierarchical_train = series[:-24]\n",
    "hierarchical_val = series[-24:-12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357806bc",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59881778",
   "metadata": {},
   "source": [
    "Let's start by defining the models we will look at. We have univariate and multivariate baselines represented with uni_ and multi_models to test. Feel free to add additional models to test. Models are expected to be from the darts package. The NBEATS parameters here were derived using Optuna and can also be adjusted as needed. For implementation details of the hyperparameter tuning process check hpo.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b76d66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_models_to_test = [\n",
    "    ARIMA(q=1),\n",
    "    ExponentialSmoothing(),\n",
    "    Prophet(),\n",
    "    NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),\n",
    "    LinearRegressionModel(lags=12),\n",
    "    XGBModel(lags=12)\n",
    "    ]\n",
    "multi_models_to_test  = [\n",
    "    NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),\n",
    "    LinearRegressionModel(lags=12),\n",
    "    XGBModel(lags=12)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ee5d4",
   "metadata": {},
   "source": [
    "Additionally, we implement here the multi-model approach outlined in our paper. From a set of suitably simple models, the best performing one is selected on a series-by-series basis. The combination of these predictions is considered a model of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ba72fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 53.67it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 276.69it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 223.65it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 191.04it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 205.72it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 266.37it/s]\n",
      "ContributionMargin1: ARIMA\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 218.74it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 248.39it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 277.00it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 244.94it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 255.84it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 167.63it/s]\n",
      "-FixCosts: ExponentialSmoothing\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 235.78it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 226.07it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 226.90it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 218.78it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 276.36it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 322.12it/s]\n",
      "NetSales: ARIMA\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 274.68it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 240.69it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 275.09it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 250.08it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 272.96it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 294.79it/s]\n",
      "-VariableCosts: ARIMA\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 286.32it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 265.03it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 237.97it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 219.79it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 259.76it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 274.60it/s]\n",
      "EBIT: ARIMA\n"
     ]
    }
   ],
   "source": [
    "simple_models_to_test = [\n",
    "    ARIMA(q=1),\n",
    "    ExponentialSmoothing(),\n",
    "    Prophet(),\n",
    "    #LinearRegressionModel(lags=12),\n",
    "    NaiveDrift(),\n",
    "    GlobalNaiveAggregate(input_chunk_length=3, output_chunk_length=3),\n",
    "    GlobalNaiveAggregate(input_chunk_length=1, output_chunk_length=1),\n",
    "    GlobalNaiveAggregate(input_chunk_length=12, output_chunk_length=12),\n",
    "    ]\n",
    "\n",
    "best_per_series = get_best_per_series(hierarchical_train, hierarchical_val,models=simple_models_to_test)\n",
    "best_per_series=best_per_series.with_hierarchy(hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b8401",
   "metadata": {},
   "source": [
    "Now, we have the actual comparison step. We pass all of our models to the compare functions. These return the fitted models and predictions for all passed model names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6034005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s, train_loss=6.63e+7]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 61.01it/s]\n",
      "MAE for ARIMA, Uni: 10593.30\n",
      "MAPE for ARIMA, Uni: 220.89\n",
      "RMSE for ARIMA, Uni: 12526.19\n",
      "R^2 for ARIMA, Uni: 0.80\n",
      "SMAPE for ARIMA, Uni: 81.98\n",
      "\n",
      "MAE for ExponentialSmoothing, Uni: 9987.80\n",
      "MAPE for ExponentialSmoothing, Uni: 188.91\n",
      "RMSE for ExponentialSmoothing, Uni: 11312.62\n",
      "R^2 for ExponentialSmoothing, Uni: 0.83\n",
      "SMAPE for ExponentialSmoothing, Uni: 84.67\n",
      "\n",
      "MAE for Prophet, Uni: 12980.65\n",
      "MAPE for Prophet, Uni: 103.01\n",
      "RMSE for Prophet, Uni: 16242.64\n",
      "R^2 for Prophet, Uni: 0.66\n",
      "SMAPE for Prophet, Uni: 117.49\n",
      "\n",
      "MAE for NBEATSModel, Uni: 7369.67\n",
      "MAPE for NBEATSModel, Uni: 98.65\n",
      "RMSE for NBEATSModel, Uni: 8942.28\n",
      "R^2 for NBEATSModel, Uni: 0.90\n",
      "SMAPE for NBEATSModel, Uni: 87.57\n",
      "\n",
      "MAE for LinearRegression, Uni: 10010.87\n",
      "MAPE for LinearRegression, Uni: 225.42\n",
      "RMSE for LinearRegression, Uni: 12464.43\n",
      "R^2 for LinearRegression, Uni: 0.80\n",
      "SMAPE for LinearRegression, Uni: 78.46\n",
      "\n",
      "MAE for XGBRegressor, Uni: 10149.12\n",
      "MAPE for XGBRegressor, Uni: 163.96\n",
      "RMSE for XGBRegressor, Uni: 13095.70\n",
      "R^2 for XGBRegressor, Uni: 0.78\n",
      "SMAPE for XGBRegressor, Uni: 85.88\n",
      "\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s, train_loss=3.97e+8]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 60.01it/s]\n",
      "MAE for NBEATSModel: 19992.12\n",
      "MAPE for NBEATSModel: 298.64\n",
      "RMSE for NBEATSModel: 23752.01\n",
      "R^2 for NBEATSModel: 0.27\n",
      "SMAPE for NBEATSModel: 111.13\n",
      "\n",
      "MAE for LinearRegression: 13287.55\n",
      "MAPE for LinearRegression: 268.58\n",
      "RMSE for LinearRegression: 14988.62\n",
      "R^2 for LinearRegression: 0.71\n",
      "SMAPE for LinearRegression: 89.97\n",
      "\n",
      "MAE for XGBRegressor: 7842.39\n",
      "MAPE for XGBRegressor: 144.25\n",
      "RMSE for XGBRegressor: 9982.81\n",
      "R^2 for XGBRegressor: 0.87\n",
      "SMAPE for XGBRegressor: 63.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare univariate baseline models, uncreconciled multivariate models, reconciled multivariate models\n",
    "# includes multi-model in the multivariate models\n",
    "fittedbaselinemodels, univariate_predictions = compare_models_univariate(train, val, uni_models_to_test)\n",
    "unreconciled_models, unreconciled_predictions = compare_models_multivariate(hierarchical_train, hierarchical_val, multi_models_to_test)\n",
    "unreconciled_predictions['Multi Model']=best_per_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cca818",
   "metadata": {},
   "source": [
    "Let's define our reconciliation methods of choice. Some must be fit before they can be applied. We check performance after reconciliation using the compare_models_reconciliated function, which is similar to the functions used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2583ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconciliator0 = MinTReconciliator(method=\"ols\")\n",
    "reconciliator0.fit(series[:-24])\n",
    "reconciliator1 = TopDownReconciliator()\n",
    "reconciliator1.fit(series[:-24])\n",
    "reconciliator2 = BottomUpReconciliator()\n",
    "reconciliators = {reconciliator0 : 'MinT',\n",
    "                  reconciliator1 : 'Top Down',\n",
    "                  reconciliator2 : 'Bottom Up'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf6b6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NBEATSModel\n",
      "Testing LinearRegression\n",
      "Testing XGBRegressor\n",
      "Testing Multi Model\n",
      "MAE for NBEATSModel, MinT: 16918.10\n",
      "MAPE for NBEATSModel, MinT: 203.98\n",
      "RMSE for NBEATSModel, MinT: 20428.10\n",
      "R^2 for NBEATSModel, MinT: 0.46\n",
      "SMAPE for NBEATSModel, MinT: 100.96\n",
      "\n",
      "MAE for NBEATSModel, Top Down: 19992.12\n",
      "MAPE for NBEATSModel, Top Down: 298.64\n",
      "RMSE for NBEATSModel, Top Down: 23752.01\n",
      "R^2 for NBEATSModel, Top Down: 0.27\n",
      "SMAPE for NBEATSModel, Top Down: 111.13\n",
      "\n",
      "MAE for NBEATSModel, Bottom Up: 20423.94\n",
      "MAPE for NBEATSModel, Bottom Up: 222.80\n",
      "RMSE for NBEATSModel, Bottom Up: 26406.72\n",
      "R^2 for NBEATSModel, Bottom Up: 0.10\n",
      "SMAPE for NBEATSModel, Bottom Up: 97.42\n",
      "\n",
      "MAE for LinearRegression, MinT: 13287.55\n",
      "MAPE for LinearRegression, MinT: 268.58\n",
      "RMSE for LinearRegression, MinT: 14988.62\n",
      "R^2 for LinearRegression, MinT: 0.71\n",
      "SMAPE for LinearRegression, MinT: 89.97\n",
      "\n",
      "MAE for LinearRegression, Top Down: 13287.55\n",
      "MAPE for LinearRegression, Top Down: 268.58\n",
      "RMSE for LinearRegression, Top Down: 14988.62\n",
      "R^2 for LinearRegression, Top Down: 0.71\n",
      "SMAPE for LinearRegression, Top Down: 89.97\n",
      "\n",
      "MAE for LinearRegression, Bottom Up: 13287.55\n",
      "MAPE for LinearRegression, Bottom Up: 268.58\n",
      "RMSE for LinearRegression, Bottom Up: 14988.62\n",
      "R^2 for LinearRegression, Bottom Up: 0.71\n",
      "SMAPE for LinearRegression, Bottom Up: 89.97\n",
      "\n",
      "MAE for XGBRegressor, MinT: 7971.98\n",
      "MAPE for XGBRegressor, MinT: 135.21\n",
      "RMSE for XGBRegressor, MinT: 9898.32\n",
      "R^2 for XGBRegressor, MinT: 0.87\n",
      "SMAPE for XGBRegressor, MinT: 67.89\n",
      "\n",
      "MAE for XGBRegressor, Top Down: 7842.39\n",
      "MAPE for XGBRegressor, Top Down: 144.25\n",
      "RMSE for XGBRegressor, Top Down: 9982.81\n",
      "R^2 for XGBRegressor, Top Down: 0.87\n",
      "SMAPE for XGBRegressor, Top Down: 63.32\n",
      "\n",
      "MAE for XGBRegressor, Bottom Up: 13566.68\n",
      "MAPE for XGBRegressor, Bottom Up: 107.86\n",
      "RMSE for XGBRegressor, Bottom Up: 16775.00\n",
      "R^2 for XGBRegressor, Bottom Up: 0.64\n",
      "SMAPE for XGBRegressor, Bottom Up: 92.75\n",
      "\n",
      "MAE for Multi Model, MinT: 33943.67\n",
      "MAPE for Multi Model, MinT: 428.53\n",
      "RMSE for Multi Model, MinT: 37312.42\n",
      "R^2 for Multi Model, MinT: -0.81\n",
      "SMAPE for Multi Model, MinT: 161.75\n",
      "\n",
      "MAE for Multi Model, Top Down: 86217.66\n",
      "MAPE for Multi Model, Top Down: 923.66\n",
      "RMSE for Multi Model, Top Down: 97097.33\n",
      "R^2 for Multi Model, Top Down: -11.22\n",
      "SMAPE for Multi Model, Top Down: 185.93\n",
      "\n",
      "MAE for Multi Model, Bottom Up: 21189.54\n",
      "MAPE for Multi Model, Bottom Up: 330.41\n",
      "RMSE for Multi Model, Bottom Up: 22850.39\n",
      "R^2 for Multi Model, Bottom Up: 0.32\n",
      "SMAPE for Multi Model, Bottom Up: 124.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reconciliatedpredictions = compare_models_reconciliated(data=hierarchical_train, val=val['EBIT'], models=unreconciled_predictions, reconciliators=reconciliators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594385a8",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001d4a5",
   "metadata": {},
   "source": [
    "Evaluations of accuracy can be found in the files titled results_var. The raw predictions can be found in the files predictions_var. For further analysis of these files, check the other notebook provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8830d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged={**unreconciled_predictions,\n",
    "**reconciliatedpredictions,}\n",
    "\n",
    "merged = {k:v['EBIT'] for k,v in merged.items()}\n",
    "\n",
    "merged = {**univariate_predictions,**merged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eca95add",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_name, ts in merged.items():\n",
    "    df = ts.to_dataframe().reset_index()  \n",
    "    df.columns = [\"Date\", \"Predictions\"]   \n",
    "    df[\"Name\"] = model_name\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged_df = merged_df[[\"Date\", \"Name\", \"Predictions\"]]\n",
    "merged_df['Date']=pd.to_datetime(merged_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edf5a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly, summary = get_winners(merged_df,val['EBIT'])\n",
    "summary.to_csv('output/results_'+hvar)\n",
    "merged_df.to_csv('output/predictions_'+hvar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
