# -*- coding: utf-8 -*-
"""hierarchical forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xXdquL9EKkLqY0x4aru26bswqytYV60q

## Setup
"""

!pip uninstall -y numpy  2>/dev/null || echo "Some packages already uninstalled"

#!pip uninstall -y numpy pandas scipy statsmodels darts statsforecast 2>/dev/null || echo "Some packages already uninstalled"

#!pip install numpy==1.26.4 --no-deps --no-cache-dir --force-reinstall --quiet # Must be first
!pip install "scipy==1.11.4" "statsmodels==0.14.1" --quiet
#!pip install darts statsforecast --quiet
#!pip install "pandas==2.2.2" "scipy==1.11.4" --force-reinstall --quiet

import numpy as np
import pandas as pd
from scipy import stats
from darts import TimeSeries
from statsforecast import StatsForecast

print("NumPy:", np.__version__)  # Should be 1.26.4
print("Pandas:", pd.__version__)  # Should be 2.2.2

from darts.models import Prophet, LinearRegressionModel, AutoARIMA, Theta, ExponentialSmoothing, XGBModel, FFT, NBEATSModel
from darts.utils.timeseries_generation import datetime_attribute_timeseries
from darts.dataprocessing.transformers import Scaler
from darts.metrics import mape, rmse, mase, r2_score, mae
from darts import concatenate
from darts.dataprocessing.transformers import MinTReconciliator, BottomUpReconciliator, TopDownReconciliator
from darts.dataprocessing.transformers import BoxCox

from pandas import Timestamp
import matplotlib.pyplot as plt

# import non-Dart libraries
from sklearn.linear_model import LinearRegression
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
from scipy.linalg import block_diag

"""### Data Import + Utils"""

import logging

logging.getLogger('prophet').setLevel(logging.WARNING)
logging.getLogger('cmdstanpy').setLevel(logging.WARNING)

def get_levels(ts):
  hierarchy = ts.hierarchy
  reversed = {}
  for key, value in ts.hierarchy.items():
    if value[0] in reversed:
      reversed[value[0]].append(key)
    else:
      reversed[value[0]] = [key]
  return reversed

### broken! don't use me yet :(

def backtestmodels(fittedmodels, data, val, univariate=False):
  backtest_results_dict = {}
  for model in fittedmodels:
    if univariate==False:
      model_name = str(re.match(r"^([A-Za-z0-9_]+)\(", str(model)).group(1)) + ", Backtested"
      backtested = model.historical_forecasts(
          series[:-12],
          start=0.6,
          forecast_horizon=12,
          stride=1,
          retrain=True
      )
      get_metrics(backtested, val, model_name)
      backtest_results_dict[model_name] = backtested
  return backtest_results_dict

def load_data(file_path='SampleHierForecastingBASF_share.xlsx'):
  df = pd.read_excel(file_path)
  df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%y')
  df = df.sort_values(by='Date')
  numeric_cols = ['EBIT', 'EBITDA', 'DepreciationAmortization', 'VariableCosts', 'NetSales', 'ContributionMargin1', 'FixCosts']
  df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
  return df



#log scaling too?
#scaled_df = stats.yeojohnson(df)

def get_metrics(pred, val, name):
  """
    Standardized performance metrics to evaluate model performance. Consider additional metrics as well.

    Args:
        pred: Predicted time series from model
        val: Validation period time series, should be same shape as pred
        name: Name of model as String

    """
  print(f"MAE for {name}: {mae(val,pred):.2f}")
  print(f"MAPE for {name}: {mape(val,pred):.2f}")
  print(f"RMSE for {name}: {rmse(val, pred):.2f}")
  print()

def get_metrics_batched(preds,val):
  """
    Standardized performance metrics to evaluate model performance. Consider adding performance evaluations accross models (ie get best performing)

    Args:
        preds: Dictionary of form name of model : predicted time series from model
        val: Validation period time series

    """
  #todo: add group evaluations to find best overall model
  for key in preds:
    print(f"MAE for {key}: {mae(val,preds[key]):.2f}")
    print(f"MAPE for {key}: {mape(val,preds[key]):.2f}")
    print(f"RMSE for {key}: {rmse(val, preds[key]):.2f}")
    print()

import re
def compare_models_simple(data, val, models, past_cov=None):
  """
    Evaluate multiple forecasting models. supports univariate and multivariate models.
    Note that this doesn't currently work with reconciliation. Do not pass data with hierarchy implemented.

    Args:
        data: Training data TimeSeries. Should consist of only target variable.
        val: Validation period time series, includes target and past_covariates
        past_covariates: Hierarchical covariates, only require

    Returns:
        Two dictionaries:
        - fitted_models: {model_name: trained_model}
        - predictions: {model_name: forecast_series}

    Example:
        >>> models = [AutoARIMA(), XGBModel(lags=12)]
        >>> fitted, preds = compare_models_simple(train, val, covariates, models)
    """
  #check performance of passed models on data, return all fitted models in case of future evaluation needs
  fittedmodels = {}
  predictions = {}
  for m in models:
    model_name = re.match(r"^([A-Za-z0-9_]+)\(", str(m)).group(1)
    if m.supports_past_covariates and past_cov:
      m.fit(data, past_covariates=past_cov)
      pred = m.predict(n=len(val), past_covariates=past_cov)
    else:
      m.fit(data)
      pred = m.predict(n=len(val))

    fittedmodels[model_name]=m
    predictions[model_name]=pred
  get_metrics_batched(predictions, val['EBIT'])
  return fittedmodels, predictions

def compare_models_reconciliated(data, val, models, reconciliator=BottomUpReconciliator()):
  """
    Evaluate multiple multivariate forecasting models with hierarchies.
    Compare performance with and without reconciliation.
    Unreconciled predictions should perform worse than in the simple models method becuase the model doesn't know which var is target during training.

    Args:
        data: Training data TimeSeries
        val: Validation period time series
        past_covariates: Hierarchical covariates, only require

    Returns:
        Two dictionaries:
        - fitted_models: {model_name: trained_model}
        - predictions: {model_name: forecast_series}

    Example:
        >>> models = [AutoARIMA(), XGBModel(lags=12)]
        >>> fitted, preds = compare_models_simple(train, val, covariates, models)
    """
  fittedmodels = {}
  predictions = {}
  for m in models:
    model_name = re.match(r"^([A-Za-z0-9_]+)\(", str(m)).group(1)
    m.fit(data)
    pred = m.predict(n=len(val))
    fittedmodels[model_name]=m
    predictions[model_name+", Unreconciled"]=pred['EBIT']
    reconciled_pred = reconciliator.transform(pred)
    predictions[model_name+", Reconciled"]=reconciled_pred['EBIT']

  get_metrics_batched(predictions, val['EBIT'])
  return fittedmodels, predictions

"""#### Implementing Hierarchy"""

def apply_hierarchy(df):
    """assumes data is in format from given sheet. change this to work with other datasets"""
    hierarchy = {
        'EBIT': ['ContributionMargin1', '-FixCosts'],  # EBIT = ContributionMargin1 - FixCosts
        'EBITDA': ['EBIT', 'DepreciationAmortization'],  # EBITDA = EBIT + DepreciationAmortization
        'ContributionMargin1': ['NetSales', '-VariableCosts'],  # ContributionMargin1 = NetSales - VariableCosts
        'NetSales': [],  # Bottom level
        '-VariableCosts': [],  # Bottom level
        '-FixCosts': [],  # Bottom level
        'DepreciationAmortization': []  # Bottom level
    }

    if isinstance(df, pd.DataFrame):
        df['-VariableCosts'] = -df['VariableCosts']
        df['-FixCosts'] = -df['FixCosts']
        series = TimeSeries.from_dataframe(df, time_col="Date", value_cols=[
            "EBIT", "EBITDA", "DepreciationAmortization", "ContributionMargin1", "-FixCosts", "NetSales", "-VariableCosts"])
        series = series.with_hierarchy(hierarchy)
        target = series["EBIT"]
        covariates = series[[
            "EBIT", "EBITDA", "DepreciationAmortization", "ContributionMargin1", "-FixCosts", "NetSales", "-VariableCosts"]]

    elif isinstance(df, TimeSeries):
        series = df.with_hierarchy(hierarchy)
        target = series["EBIT"]
        covariates = series[[
            "EBIT", "EBITDA", "DepreciationAmortization", "ContributionMargin1", "-FixCosts", "NetSales", "-VariableCosts"]]

    else:
        raise TypeError("df must be a pandas DataFrame or TimeSeries")

    return series, target, covariates

"""#### Reconciliation"""

def plot_forecasts(actual, predicted, title, path="plots"):
    """Plot actual vs predicted values"""
    plt.figure(figsize=(12, 6))
    plt.plot(actual, label='Actual', marker='o')
    plt.plot(predicted, label='Predicted', marker='s')
    plt.title(f'{title} - Actual vs Predicted')
    plt.legend()
    plt.tight_layout()
    plt.show()
    # plt.savefig(f"{path}/{title.replace(' ', '_')}.png")
    # plt.close()



def create_summing_matrix_path1():
    """
    Create the summing matrix S for hierarchical path 1:
    EBIT = ContributionMargin1 - FixCosts
    ContributionMargin1 = NetSales - VariableCosts
    """
    # Number of series at the bottom level
    m = 3  # NetSales, -VariableCosts, -FixCosts
    # Total number of series in hierarchy
    n = 4  # EBIT + ContributionMargin1 + Bottom level (2)

    S = np.zeros((n, m))

    # EBIT = NetSales - VariableCosts - FixCosts
    S[0] = np.array([1, 1, 1])

    # ContributionMargin1 = NetSales - VariableCosts
    S[1] = np.array([1, 1, 0])

    # Bottom level identity matrix for NetSales and -VariableCosts
    S[2] = np.array([1, 0, 0])  # NetSales
    S[3] = np.array([0, 1, 0])  # -VariableCosts

    print("Summing Matrix S for Path 1 (ContributionMargin1):")
    print(S)
    print(f"Shape of S1: {S.shape}")
    return S


def create_summing_matrix_path2():
    """
    Create the summing matrix S for hierarchical path 2:
    EBIT = EBITDA - DepreciationAmortization
    """
    # Number of series at the bottom level
    m = 2  # EBITDA, -DepreciationAmortization
    # Total number of series in hierarchy
    n = 3  # EBIT + Bottom level (2)

    S = np.zeros((n, m))

    # EBIT = EBITDA - DepreciationAmortization
    S[0] = np.array([1, 1])

    # Bottom level identity matrix
    S[1] = np.array([1, 0])   # EBITDA
    S[2] = np.array([0, 1])   # -DepreciationAmortization

    print("\nSumming Matrix S for Path 2 (EBITDA):")
    print(S)
    print(f"Shape of S2: {S.shape}")
    return S

def create_block_diagonal_matrix():
    """
    Create a block diagonal matrix combining both hierarchical paths.
    This ensures that the two hierarchies are treated independently in the reconciliation process.
    """
    S1 = create_summing_matrix_path1()
    S2 = create_summing_matrix_path2()

    print("\nDebugging matrix shapes:")
    print(f"S1 shape: {S1.shape}")
    print(f"S2 shape: {S2.shape}")

    # Create block diagonal matrix
    S_block = block_diag(S1, S2)

    print("\nBlock Diagonal Summing Matrix:")
    print(S_block)
    print(f"Shape of block diagonal matrix: {S_block.shape}")
    return S_block


def load_and_prepare_data(file_path):
    """Load and prepare the data"""
    df = pd.read_excel(file_path)
    df['Date'] = pd.to_datetime(df['Date'])

    # Create the negative columns
    df['-VariableCosts'] = -df['VariableCosts']
    df['-DepreciationAmortization'] = -df['DepreciationAmortization']
    df['-FixCosts'] = -df['FixCosts']

    # Split into train and test
    train = df[df['Date'].dt.year < 2022]
    test = df[df['Date'].dt.year == 2022]

    return train, test

def create_time_series_dict(train_df, test_df):
    """Create dictionary of numpy arrays for each component"""
    series_dict = {}
    columns = ['EBIT', 'ContributionMargin1', 'EBITDA',
               'NetSales', '-VariableCosts', '-FixCosts', '-DepreciationAmortization']

    for col in columns:
        # Training series
        series_dict[col] = train_df[col].values
        # Test series (for evaluation)
        series_dict[f"{col}_test"] = test_df[col].values
    return series_dict

def forecast_with_models(series, n_periods=12):
    """Generate forecasts using both Linear Regression and Exponential Smoothing"""
    # Prepare data for Linear Regression
    X = np.arange(len(series)).reshape(-1, 1)
    y = series.reshape(-1, 1)

    # Linear Regression forecast
    lr_model = LinearRegression()
    lr_model.fit(X, y)
    X_future = np.arange(len(series), len(series) + n_periods).reshape(-1, 1)
    lr_forecast = lr_model.predict(X_future).flatten()

    # Exponential Smoothing forecast
    exp_model = ExponentialSmoothing(series)
    exp_fit = exp_model.fit()
    exp_forecast = exp_fit.forecast(n_periods)

    return lr_forecast, exp_forecast

def plot_forecasts_ts(actual, predicted, title, dates, path="plots"):
    """Plot actual vs predicted values with proper time series structure"""
    plt.figure(figsize=(12, 6))

    # Convert to pandas Series with datetime index
    actual_series = pd.Series(actual, index=dates)
    predicted_series = pd.Series(predicted, index=dates)

    plt.plot(actual_series, color='black', label='Actual EBIT', linewidth=2)
    plt.plot(predicted_series, color='blue', label='Predicted EBIT', linewidth=2)

    plt.title(title)
    plt.xlabel('time')
    plt.ylabel('Value')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


def load_and_prepare_data(file_path):
    """Load and prepare the data"""
    df = pd.read_excel(file_path)
    df['Date'] = pd.to_datetime(df['Date'])

    # Create the negative columns
    df['-VariableCosts'] = -df['VariableCosts']
    df['-DepreciationAmortization'] = -df['DepreciationAmortization']
    df['-FixCosts'] = -df['FixCosts']

    # Split into train and test
    train = df[df['Date'].dt.year < 2022]
    test = df[df['Date'].dt.year == 2022]

    return train, test

def create_time_series_dict(train_df, test_df):
    """Create dictionary of numpy arrays for each component"""
    series_dict = {}
    columns = ['EBIT', 'ContributionMargin1', 'EBITDA',
               'NetSales', '-VariableCosts', '-FixCosts', '-DepreciationAmortization']

    for col in columns:
        # Training series
        series_dict[col] = train_df[col].values
        # Test series (for evaluation)
        series_dict[f"{col}_test"] = test_df[col].values
    return series_dict

def forecast_with_models(series, dates, n_periods=12):
    """Generate forecasts using Linear Regression, Exponential Smoothing, and ARIMA"""
    # Prepare data for Linear Regression
    X = np.arange(len(series)).reshape(-1, 1)
    y = series.reshape(-1, 1)

    # Linear Regression forecast
    lr_model = LinearRegression()
    lr_model.fit(X, y)
    X_future = np.arange(len(series), len(series) + n_periods).reshape(-1, 1)
    lr_forecast = lr_model.predict(X_future).flatten()

    # Exponential Smoothing forecast
    exp_model = ExponentialSmoothing(series)
    exp_fit = exp_model.fit()
    exp_forecast = exp_fit.forecast(n_periods)

    # ARIMA forecast
    # Start with a simple ARIMA(1,1,1) model
    try:
        arima_model = ARIMA(series, order=(1,1,1))
        arima_fit = arima_model.fit()
        arima_forecast = arima_fit.forecast(n_periods)
    except:
        print(f"ARIMA failed, using simple moving average as fallback")
        # Fallback to simple moving average if ARIMA fails
        arima_forecast = np.array([np.mean(series[-12:])] * n_periods)

    return lr_forecast, exp_forecast, arima_forecast

# TopDown
def compute_topdown_proportions(train_df):
    """
    Compute average proportions of bottom-level series relative to top-level aggregates
    Returns a dictionary with proportions for each path
    """
    proportions = {}

    # Path 1: NetSales, -VariableCosts, -FixCosts relative to EBIT
    top1 = train_df['EBIT']
    bottom1 = train_df[['NetSales', '-VariableCosts', '-FixCosts']]
    proportions['path1'] = (bottom1.T / top1).mean(axis=1).values

    # Path 2: EBITDA, -DepreciationAmortization relative to EBIT
    bottom2 = train_df[['EBITDA', '-DepreciationAmortization']]
    proportions['path2'] = (bottom2.T / top1).mean(axis=1).values

    return proportions

def create_topdown_G_matrix(proportions):
    """
    Create a top-down disaggregation matrix from proportions
    Returns G matrix of shape (n_bottom, n_total)
    """
    n_total = 7  # EBIT, ContributionMargin1, EBITDA, NetSales, -VariableCosts, -FixCosts, -DepreciationAmortization
    n_bottom = 5

    G = np.zeros((n_bottom, n_total))

    # Map EBIT forecasts to bottom levels using proportion
    # EBIT is index 0 in the series_dict, bottom levels start from index 3
    # Row = bottom, Column = top (we only use column 0 — EBIT)
    # NetSales, -VariableCosts, -FixCosts
    G[0, 0] = proportions['path1'][0]  # NetSales
    G[1, 0] = proportions['path1'][1]  # -VariableCosts
    G[2, 0] = proportions['path1'][2]  # -FixCosts
    G[3, 0] = proportions['path2'][0]  # EBITDA
    G[4, 0] = proportions['path2'][1]  # -DepreciationAmortization

    print("\nTop-Down G Matrix:")
    print(G)
    print(f"Shape of G: {G.shape}")
    return G

def create_g_matrix_bottom_up(n_total, n_bottom):
    """
    Create the G matrix for bottom-up approach.
    Parameters:
    - n_total: total number of series (including aggregated levels)
    - n_bottom: number of bottom-level series
    Returns:
    - G matrix of shape (n_bottom, n_total)
    """
    # Zero matrix of shape (n_bottom, n_total)
    G = np.zeros((n_bottom, n_total))

    # Fill in identity matrix in the bottom-right corner
    G[:, -n_bottom:] = np.identity(n_bottom)

    print("\nBottom-Up G Matrix:")
    print(G)
    print(f"Shape of G: {G.shape}")
    return G

def disaggregate_forecast(top_forecast, G):
    """
    Multiply top-level forecast by G to get bottom-level forecasts over time.
    top_forecast: (n_periods,) or (n_periods, 1)
    G: (n_bottom, n_total)

    Returns:
    bottom_forecast: shape (n_bottom, n_periods)
    """
    # Make sure it's a 1D array
    top_forecast = top_forecast.flatten()

    # Only one top-level forecast? (single series over time)
    # So we tile it across the number of bottom series
    # We treat the top forecast as if it's a (1, n_periods) row vector
    return (G @ np.concatenate([top_forecast[np.newaxis, :], np.zeros((G.shape[1]-1, len(top_forecast)))], axis=0))



"""# Models

### Setup for Model Comparison
"""

df = load_data()
series, target, covariates = apply_hierarchy(df)

"""#### Seasonal Adjustments"""

#add seasonal differencing

"""#### Adding Covariates"""

train, val = target[:-24], series[-24:-12]
past_cov = covariates[:-12]

#yearly global inflation data from world bank, extra explanatory variable for model
inflation = pd.read_excel('globalinflation.xlsx')
inflation = inflation.T
inflation.index = pd.to_datetime(inflation.index, format='%Y')

# data is yearly, make it monthly
start_date = inflation.index.min()
end_date = min(inflation.index.max() + pd.DateOffset(months=11),past_cov.time_index.max() )
date_range = pd.date_range(start=start_date, end=end_date, freq='MS')
inflation = inflation.reindex(date_range)
#inflation = inflation.ffill()
inflation = inflation.interpolate(method="linear")
inflation = inflation.reset_index()
inflation.columns = ["Date","inflation_rate"]

# now can be used in past_cov
past_cov = past_cov.stack(TimeSeries.from_dataframe(inflation, time_col="Date", value_cols="inflation_rate"))

#add lag features as past_cov(?)

"""### Baseline Model Comparison"""

#get baseline performance, 2022 is validation year

models_to_test = [
    AutoARIMA(lags=12, lags_past_covariates=12),
    #ExponentialSmoothing(),
    Prophet(),
    FFT(),
    #NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),
    LinearRegressionModel(lags=12, lags_past_covariates=12),
    XGBModel(lags=12, lags_past_covariates=12)
    ]

fittedbaselinemodels, predictions = compare_models_simple(train, val, models_to_test, past_cov)

#get reconciliated performance compared to baseline multivariate, 2022 is validation year
hierarchical_train = series[:-24]
hierarchical_val = series[-24:-12]

models_to_test = [

   # NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),
    LinearRegressionModel(lags=12),
    XGBModel(lags=12)
    ]

reconciliator0 = MinTReconciliator(method="ols")
reconciliator0.fit(series[:-24])

reconciliatedmodels, reconciliatedpredictions = compare_models_reconciliated(series[:-24], val['EBIT'], models_to_test, reconciliator=reconciliator0)

"""### Model Training (Exponential Smoothing with Backtesting)"""

train, val = series[:-24], series[-24:-12]


series_dict = {
    "EBIT": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBIT"]),
    "EBITDA": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBITDA"]),
    "-DepreciationAmortization":  TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-DepreciationAmortization"]),
    "ContributionMargin1" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["ContributionMargin1"]),
    "-FixCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-FixCosts"]),
    "NetSales": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["NetSales"]),
    "-VariableCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-VariableCosts"]),
}


backtest_results_dict = {}  # Dictionary to store individual component backtests

# what if we do exponential smoothing on everything?
#check that this method can handle seasonality. htat might improve it.
for key in series_dict.keys():
    model = ExponentialSmoothing()
    model.fit(series_dict[key][:-12])  # Fit to training data
    backtest_results_dict[key] = model.historical_forecasts(
        series_dict[key][:-12],
        start=0.6,
        forecast_horizon=12,
        stride=1,
        retrain=True
    )
# Combine the individual backtest results into a single TimeSeries
backtest_results = concatenate(list(backtest_results_dict.values()), axis=1)

# Add the hierarchy to backtest_results (if you need it for reconciliation)
backtest_results = backtest_results.with_hierarchy(hierarchy)

# Now apply your reconciliation method (like BottomUpReconciliator)
reconciled_backtest_results = reconciliator1.transform(backtest_results)

get_metrics(backtest_results['EBIT'],val['EBIT'], "Exponential Smoothing with Backtest")

#again, reconciliation lowers performance(!)
get_metrics(reconciled_backtest_results['EBIT'], val['EBIT'], "Exponential Smoothing with Backtest, Reconciled")

"""### Model Training (Separate Series)"""

oracle_models = {'EBIT': LinearRegressionModel(lags=12),
 'EBITDA': XGBModel(lags=12),
 'DepreciationAmortization': LinearRegressionModel(lags=12),
 'ContributionMargin1': Prophet(),
 '-FixCosts': Prophet(),
 'NetSales': LinearRegressionModel(lags=12),
 '-VariableCosts': LinearRegressionModel(lags=12)}

series_dict = {
    "EBIT": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBIT"]),
    "EBITDA": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBITDA"]),
    "DepreciationAmortization":  TimeSeries.from_dataframe(df, time_col="Date", value_cols=["DepreciationAmortization"]),
    "ContributionMargin1" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["ContributionMargin1"]),
    "-FixCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-FixCosts"]),
    "NetSales": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["NetSales"]),
    "-VariableCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-VariableCosts"]),
}

forecasts_dict = {}
for key in series_dict.keys():
  forecasts_dict[key] = []
  m = oracle_models[key]
  m.fit(series_dict[key][:-24])
  forecasts_dict[key]=m.predict(n=len(val))

forecasts_dict['EBITDA']

#how did we do? plot to see
plt.figure(figsize=(10, 8))
#filter so we only show dates we actually calculated predictions for
filtered_series = series.drop_after(Timestamp('2022-12-31'))
filtered_series[list(forecasts_dict.keys())].plot(lw=1, color='gray', label='Actual')
for key, f in forecasts_dict.items():
    f.plot(lw=1, color='red', label=f'Predicted - {key}')  # Label with the key
plt.legend(["Series", "Prediction"], loc="upper left", labelcolor=["gray","red"])
plt.show()

forecasts = concatenate(list(forecasts_dict.values()), axis=1)

forecasts

forecasts, temp1, temp2 = apply_hierarchy(forecasts)

forecast_df = pd.DataFrame()
for key, ts in forecasts_dict.items():
    # Convert the TimeSeries to a DataFrame with a 'Date' index and the values in a column named after the key
    temp_df = ts.pd_dataframe()
    temp_df.columns = [key]  # Rename the value column to the key

    # Merge the temporary DataFrame into the main DataFrame
    if forecast_df.empty:
        forecast_df = temp_df  # Initialize if empty
    else:
        forecast_df = forecast_df.merge(temp_df, on='Date', how='outer')

forecasts['EBIT']

reconciliator0 = MinTReconciliator(method="ols")
reconciliator0.fit(series[:-24])
reconciliator1 = TopDownReconciliator()
reconciliator1.fit(series[:-24])
reconciliator2 = BottomUpReconciliator()

reconciled0_forecasts = reconciliator0.transform(forecasts)
get_metrics(reconciled0_forecasts, val, "MiNT, Forecasts Predicted Separately")
reconciled1_forecasts = reconciliator1.transform(forecasts)
#get_metrics(reconciled1_forecasts, val, "TopDown, Forecasts Predicted Separately")
reconciled2_forecasts = reconciliator2.transform(forecasts)
get_metrics(reconciled2_forecasts, val, "BottomUp, Forecasts Predicted Separately")

get_metrics(forecasts, val, "Unreconciled, Forecasts Predicted Separately")

get_metrics(reconciled0_forecasts['EBIT'], val['EBIT'], "MiNT, Forecasts Predicted Separately, Target Variable")
#get_metrics(reconciled1_forecasts, val, "TopDown, Forecasts Predicted Separately")
#reconciled2_forecasts = reconciliator2.transform(forecasts)
get_metrics(reconciled2_forecasts['EBIT'], val['EBIT'], "BottomUp, Forecasts Predicted Separately, Target Variable")
get_metrics(forecasts['EBIT'], val['EBIT'], "Unreconciled, Forecasts Predicted Separately, Target Variable")

"""#### Backtesting"""

#now the version using separately trained time series AND backtesting
#todo: compare models for each covariate to find the best one


# Assuming forecasts_dict holds your individual forecasts (key: component, value: TimeSeries)
backtest_results_dict = {}  # Dictionary to store individual component backtests

# Loop 1: Backtesting with Exponential Smoorhing for EBIT and EBITDA
for key in ["EBIT", "EBITDA"]:
    model = AutoARIMA(lags=12)  # Assuming AutoARIMA for these components
    model.fit(series_dict[key][:-12])  # Fit to training data
    backtest_results_dict[key] = model.historical_forecasts(
        series_dict[key][:-12],
        start=0.6,
        forecast_horizon=6,
        stride=1,
        retrain=True
    )

# Loop 2: Backtesting with Linear Regression for other series
for key in forecasts_dict.keys():
    if key not in ["EBIT", "EBITDA"]:  # Exclude EBIT and EBITDA
        model = LinearRegressionModel(lags=3)  # Assuming Linear Regression
        model.fit(series_dict[key][:-12])  # Fit to training data
        backtest_results_dict[key] = model.historical_forecasts(
            series_dict[key][:-12],
            start=0.6,
            forecast_horizon=3,
            stride=1,
            retrain=True
        )
# Combine the individual backtest results into a single TimeSeries
backtest_results = concatenate(list(backtest_results_dict.values()), axis=1)

# Add the hierarchy to backtest_results (if you need it for reconciliation)
backtest_results = backtest_results.with_hierarchy(hierarchy)

# Now apply your reconciliation method (like BottomUpReconciliator)
reconciled_backtest_results = reconciliator1.transform(backtest_results)

# prompt: evaluate prediction performance across hierarchy levels

def evaluate_prediction_performance_hierarchy(pred, val, hierarchy):
  """
  Evaluates prediction performance across different levels of a hierarchy.

  Args:
      pred: TimeSeries object with predictions.
      val: TimeSeries object with actual values.
      hierarchy: Dictionary representing the hierarchy of components.
  """

  # Evaluate overall performance
  calculate_metrics_for_level(pred, val, "Overall")


  # Iterate through hierarchy levels
  for level_name, components in hierarchy.items():
      pred_level = pred.filter(level_name)
      val_level = val.filter(level_name)
      print(level_name)
      get_metrics(pred_level, val_level, level_name)


# Example usage:
# Assuming you have pred and val as TimeSeries objects and hierarchy defined as above
evaluate_prediction_performance_hierarchy(pred, val, hierarchy)

plt.figure(figsize=(10, 6))
filtered_series["EBIT"].plot(label="Actual EBIT")
reconciled_backtest_results_multimodel["EBIT"].plot(label="Predicted EBIT")
plt.legend()
plt.title("Multimodel, Backtested, with Reconciliation")
plt.show()

get_metrics(backtest_results, val, "Multi Model with Backtest, Unreconciled")
print()
get_metrics(reconciled_backtest_results, val, "Multi Model with Backtest, Reconciled")
reconciled_backtest_results_multimodel = reconciled_backtest_results

"""# Reconciliation Methods"""

lr_preds =  LinearRegressionModel(lags=12).fit(hierarchical_train).predict(len(hierarchical_val))
xgb_preds = XGBModel(lags=12).fit(hierarchical_train).predict(len(hierarchical_val))

reconciliator0 = MinTReconciliator(method="ols")
reconciliator0.fit(hierarchical_train)
reconciliator1 = TopDownReconciliator()
reconciliator1.fit(hierarchical_train)
reconciliator2 = BottomUpReconciliator()

reconciled0_preds = reconciliator0.transform(xgb_preds)
reconciledlr_preds = reconciliator1.transform(lr_preds)
reconciled1_preds = reconciliator1.transform(xgb_preds)

get_metrics(reconciled0_preds['EBIT'], hierarchical_val['EBIT'], "MinT, XGB, only target")
get_metrics(reconciled1_preds['EBIT'], hierarchical_val['EBIT'], "TopDown, XGB, only target")
get_metrics(xgb_preds['EBIT'], hierarchical_val['EBIT'], "XGB, only target")

get_metrics(reconciled0_preds, hierarchical_val, "MinT, XGB, whole model")
get_metrics(reconciled1_preds, hierarchical_val, "TopDown, XGB, whole model")
get_metrics(xgb_preds, hierarchical_val, "XGB, whole model")

#interested to see error of the different forecasts before and after reconciliation

import matplotlib.pyplot as plt

# Example data (replace with your actual errors)

cols = xgb_preds.columns
base_rmse = [rmse(xgb_preds[s], hierarchical_val[s]) for s in cols]
reconciled_rmse = [rmse(reconciled0_preds[s], hierarchical_val[s]) for s in cols]

plot_df = pd.DataFrame({"Series": cols, "Base RMSE": base_rmse, "Reconciled RMSE": reconciled_rmse})

plt.figure(figsize=(10, 5))
ax = plot_df.plot(x="Series", y=["Base RMSE", "Reconciled RMSE"], kind="bar", color=["skyblue", "salmon"])
plt.title("RMSE Before vs. After Reconciliation")
plt.ylabel("RMSE")
plt.xticks(rotation=45)
for p in ax.patches:
    ax.annotate(f"{p.get_height():.1f}", (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')
plt.show()

plt.figure(figsize=(10, 6))
reconciledlr_preds["EBIT"].plot(label="Predicted with Reconciliation", color="blue")
lr_preds["EBIT"].plot(label="Predicted without Reconciliation", color="red")
series.drop_before(Timestamp('2019-01-01')).drop_after((Timestamp('2023-01-01')))["EBIT"].plot(label="Actual EBIT", color="black")


plt.legend()
plt.title("LR Model, Backtested, with and without Reconciliation")
plt.show()

"""# Kat Experiments

### Hierarchical Structure analysis
"""

# 1. Check ACTUAL DATA hierarchy violations
def check_hierarchy(actuals: pd.DataFrame):
    if 'FixCosts' in actuals.columns:
      violations = {
          'EBITDA_EBIT_Dep': actuals['EBITDA'] - (actuals['EBIT'] + actuals['DepreciationAmortization']),
          'EBIT_CM_FixCosts': actuals['EBIT'] - (actuals['ContributionMargin1'] - actuals['FixCosts']),
          'CM_Sales_VarCosts': actuals['ContributionMargin1'] - (actuals['NetSales'] - actuals['VariableCosts'])
      }
    else:
      violations = {
          'EBITDA_EBIT_Dep': actuals['EBITDA'] - (actuals['EBIT'] + actuals['DepreciationAmortization']),
          'EBIT_CM_FixCosts': actuals['EBIT'] - (actuals['ContributionMargin1'] + actuals['-FixCosts']),
          'CM_Sales_VarCosts': actuals['ContributionMargin1'] - (actuals['NetSales'] + actuals['-VariableCosts'])
      }

    return pd.DataFrame(violations)

# 2. Check FORECAST hierarchy violations
def check_forecast_violations(forecasts: pd.DataFrame):
    return check_hierarchy(forecasts)  # Same logic as above

# 3. Anti-Reconciliation Test (Run this after reconciliation)
def anti_reconciliation_test(base_forecasts, reconciled_forecasts, actuals):
    results = {}
    for col in actuals.columns:
        base_error = np.mean(np.abs(base_forecasts[col] - actuals[col]))
        recon_error = np.mean(np.abs(reconciled_forecasts[col] - actuals[col]))
        results[col] = {'BaseError': base_error, 'ReconError': recon_error, 'Worsened': recon_error > base_error}
    return pd.DataFrame(results).T

shortened = df[df['Date'] <= Timestamp('2022-12-31')]

check_hierarchy(shortened).head()

check_hierarchy(forecast_df).head()

"""#### Forecast Window/Hyperparameter Tests"""

!pip install "optuna<4.0.0" --quiet

import optuna
from optuna.pruners import MedianPruner

def objective(trial):
    params = {
        'input_chunk_length': trial.suggest_categorical('input_chunk_length', [12, 24, 36]),
        'output_chunk_length': trial.suggest_categorical('output_chunk_length', [6, 12, 24]),
        'dropout': trial.suggest_float('dropout', 0.0, 0.3),
        'n_epochs': trial.suggest_int('n_epochs', 10, 50),
        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
    }

    model = NBEATSModel(**params,
                        pl_trainer_kwargs={
            'enable_progress_bar': False  # Reduce output clutter
        })

    try:
        model.fit(
            train,
            past_covariates=past_cov,
            epochs=params['n_epochs'],
            verbose=False
        )

        # Report intermediate score (enables pruning)
        pred = model.predict(len(val))
        mape_score = mape(val['EBIT'], pred)
        trial.report(mape_score, step=model.epochs_trained)

        # Handle pruning
        if trial.should_prune():
            raise optuna.TrialPruned()

        return mape_score

    except Exception as e:
        print(f"Trial failed: {str(e)}")
        raise optuna.TrialPruned()



study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30)

print("Best params:", study.best_params)
#Best params: {'input_chunk_length': 36, 'output_chunk_length': 24, 'dropout': 0.11891699976631348, 'n_epochs': 27, 'batch_size': 128}

#]what is the best forecasting model for regression model?
for i in [1,3,6,12]:
  backtest_results = model.historical_forecasts(
      series[:-12],
      #past_covariates=past_cov,
      start=0.6,  # Start backtesting after 60% of the dataset
      forecast_horizon=i,
      stride=1,  # Move forward one step at a time
      retrain=True,  # Retrain after each forecast
  )
  get_metrics(backtest_results, val, "LR with Backtest, forecast horizon "+str(i))
  print()

#answer appears to be 3

#best forecast horizon for ExponentialSmoothing and Prophet
for i in [6,12,24,36]:
  backtest_results = ExponentialSmoothing().historical_forecasts(
      series['EBIT'][:-12],
      #past_covariates=past_cov,
      start=0.6,  # Start backtesting after 60% of the dataset
      forecast_horizon=i,
      stride=1,  # Move forward one step at a time
      retrain=True,  # Retrain after each forecast
  )
  get_metrics(backtest_results, val['EBIT'], "Exponential Smoothing, forecast horizon "+str(i))
  print()

#best forecast_horizon found to be 12 or 24

#for i in [1,3,6,12]:
 # backtest_results = Prophet().historical_forecasts(
  #    series['EBIT'][:-12],
      #past_covariates=past_cov,
   #   start=0.6,  # Start backtesting after 60% of the dataset
    #  forecast_horizon=i,
     # stride=1,  # Move forward one step at a time
      #retrain=True,  # Retrain after each forecast
  #)
  #get_metrics(backtest_results, val['EBIT'], "Prophet, forecast horizon "+str(i))
  #print()

#12 or 24 months best

"""### best models per series"""



import re
models_to_test = [
    AutoARIMA(lags=12),
    #ExponentialSmoothing(),
    Prophet(),
    FFT(),
    LinearRegressionModel(lags=12),
    XGBModel(lags=12)
    ]


series_dict = {
    "EBIT": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBIT"]),
    "EBITDA": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBITDA"]),
    "DepreciationAmortization":  TimeSeries.from_dataframe(df, time_col="Date", value_cols=["DepreciationAmortization"]),
    "ContributionMargin1" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["ContributionMargin1"]),
    "-FixCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-FixCosts"]),
    "NetSales": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["NetSales"]),
    "-VariableCosts" : TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-VariableCosts"]),
}


# Train models separately for each series
forecasts_dict = {}
for key in series_dict.keys():
  forecasts_dict[key] = []
  for model in models_to_test:
    print(model)
    model_name = re.match(r"^([A-Za-z0-9_]+)\(", str(model)).group(1)
    model.fit(series_dict[key][:-24])
    forecasts_dict[key].append({model_name:model.predict(n=len(val))})

#forecasts = concatenate(list(forecasts_dict.values()), axis=1)

best_models_rmse = {}
best_models_mae = {}
best_models_mape = {}
for s in series_dict.keys():
  print(s)
  print("processing... ")
  #get best performing model
  v = series_dict[s][-24:-12]
  current = forecasts_dict[s]
  best_rmse = float('inf')
  best_mae = float('inf')
  best_mape = float('inf')
  for model_data in current:
    for name, f in model_data.items():
      if rmse(v,f) < best_rmse:
        best_models_rmse[s] = name
        best_rmse = rmse(v,f)
      if mae(v,f) < best_mae:
        best_models_mae[s] = name
        best_mae = mae(v,f)
      if mape(v,f) < best_mape:
        best_models_mape[s] = name
        best_mape = mape(v,f)

# prompt: select most frequent string from list

from collections import Counter

def most_frequent_string(string_list):
  """
  Finds the most frequent string in a list of strings.

  Args:
    string_list: A list of strings.

  Returns:
    The most frequent string in the list. Returns None if the list is empty.
  """
  if not string_list:
    return None

  string_counts = Counter(string_list)
  return string_counts.most_common(1)[0][0]

best_overall_models = {}
for s in best_models_rmse.keys():
  if best_models_mae[s] == best_models_mape[s]:
    best_overall_models[s] = best_models_mape[s]
  else:
    best_overall_models[s] = best_models_rmse[s]

best_overall_models

"""# Draft - J1

### Reconciliation self-implementation: BottomUp
"""

# Load data
train_df, test_df = load_and_prepare_data('SampleHierForecastingBASF_share.xlsx')
series_dict = create_time_series_dict(train_df, test_df)

# Create date ranges for plotting
train_dates = train_df['Date']
test_dates = test_df['Date']

# Create block diagonal matrix combining both hierarchies
S_block = create_block_diagonal_matrix()
G = create_g_matrix_bottom_up(n_total=7, n_bottom=5)

# Define bottom level series for both paths
bottom_series_path1 = ['NetSales', '-VariableCosts', '-FixCosts']  # Path 1
bottom_series_path2 = ['EBITDA', '-DepreciationAmortization']      # Path 2

# Generate forecasts for both paths separately
base_forecasts_lr_path1 = []
base_forecasts_exp_path1 = []
base_forecasts_arima_path1 = []
base_forecasts_lr_path2 = []
base_forecasts_exp_path2 = []
base_forecasts_arima_path2 = []

# Generate forecasts for Path 1
print("\nGenerating forecasts for Path 1...")
for series_name in bottom_series_path1:
  print(f"Forecasting {series_name}")
  series_data = series_dict[series_name]  # Get the actual series data
  lr_forecast, exp_forecast, arima_forecast = forecast_with_models(series_data, train_dates)
  base_forecasts_lr_path1.append(lr_forecast)
  base_forecasts_exp_path1.append(exp_forecast)
  base_forecasts_arima_path1.append(arima_forecast)

# Generate forecasts for Path 2
print("\nGenerating forecasts for Path 2...")
for series_name in bottom_series_path2:
  print(f"Forecasting {series_name}")
  series_data = series_dict[series_name]  # Get the actual series data
  lr_forecast, exp_forecast, arima_forecast = forecast_with_models(series_data, train_dates)
  base_forecasts_lr_path2.append(lr_forecast)
  base_forecasts_exp_path2.append(exp_forecast)
  base_forecasts_arima_path2.append(arima_forecast)

# Stack and reconcile forecasts using block diagonal matrix
print("\nReconciling forecasts using block diagonal matrix...")
# Linear Regression

# Extract only the values, not the entire TimeSeries object
base_stacked_lr_path1 = np.column_stack(base_forecasts_lr_path1)  # Shape: (n_periods, 3)
base_stacked_lr_path2 = np.column_stack(base_forecasts_lr_path2)  # Shape: (n_periods, 2)  # Shape: (n_periods, 2)
base_stacked_exp_path1 = np.column_stack(base_forecasts_exp_path1)
base_stacked_exp_path2 = np.column_stack(base_forecasts_exp_path2)
base_stacked_arima_path1 = np.column_stack(base_forecasts_arima_path1)
base_stacked_arima_path2 = np.column_stack(base_forecasts_arima_path2)


# Combine paths for each model
base_stacked_lr = np.column_stack([base_stacked_lr_path1, base_stacked_lr_path2])
base_stacked_exp = np.column_stack([base_stacked_exp_path1, base_stacked_exp_path2])
base_stacked_arima = np.column_stack([base_stacked_arima_path1, base_stacked_arima_path2])

# Reconcile forecasts
reconciled_lr = S_block @ base_stacked_lr.T
reconciled_exp = S_block @ base_stacked_exp.T
reconciled_arima = S_block @ base_stacked_arima.T

# Evaluate and plot results
print("\nEvaluation of reconciled forecasts:")

# Define series names for both paths
path1_series = ['EBIT', 'ContributionMargin1', 'NetSales', '-VariableCosts']
path2_series = ['EBIT', 'EBITDA', '-DepreciationAmortization']

# Evaluate Path 1 results
print("\nPath 1 Results:")
for i, series_name in enumerate(path1_series):
  print(f"\nEvaluating {series_name} (Path 1):")
  # Get test data for evaluation (use only first 12 months if test period is longer)
  test_data = series_dict[f"{series_name}_test"][:12]

  # Linear Regression
  print("Linear Regression:")
  get_metrics(reconciled_lr[i], test_data, f"LR {series_name}")
  plot_forecasts(test_data, reconciled_lr[i], f"{series_name} LR Path1", test_dates[:12])

  # Exponential Smoothing
  print("Exponential Smoothing:")
  get_metrics(reconciled_exp[i], test_data, f"EXP {series_name}")
  plot_forecasts(test_data, reconciled_exp[i], f"{series_name} EXP Path1", test_dates[:12])

  # ARIMA
  print("ARIMA:")
  get_metrics(reconciled_arima[i], test_data, f"ARIMA {series_name}")
  plot_forecasts(test_data, reconciled_arima[i], f"{series_name} ARIMA Path1", test_dates[:12])

print(reconciled_lr[0])

import pandas as pd

def generate_and_print_forecast_for_model(model_name, path1_series, test_dates, forecast_matrix):
    """
    Generates and prints forecast DataFrame for a given model.

    Parameters:
    - model_name (str): Name of the model (e.g., 'LR', 'EXP', 'ARIMA')
    - path1_series (list): List of series names
    - test_dates (list or array): List of forecast dates
    - forecast_matrix (2D array): Forecast values [series, month]
    """

    forecast_rows = []

    # Only take first 12 months
    test_dates_12 = test_dates[:12]

    for i, series_name in enumerate(path1_series):
        for month_idx, forecast_date in enumerate(test_dates_12):
            forecast_rows.append({
                "Date": forecast_date,
                "Series": series_name,
                "Forecast Month": month_idx + 1,
                "Forecast Value": forecast_matrix[i, month_idx]
            })

    # Convert to DataFrame and print
    df = pd.DataFrame(forecast_rows)
    df["Forecast Value"] = df["Forecast Value"].round(2)
    print(f"\nForecasts for model: {model_name}")
    print(df)

generate_and_print_forecast_for_model("LR", path1_series, test_dates, reconciled_lr)
generate_and_print_forecast_for_model("EXP", path1_series, test_dates, reconciled_exp)
generate_and_print_forecast_for_model("ARIMA", path1_series, test_dates, reconciled_arima)

# Evaluate Path 2 results
print("\nPath 2 Results:")
offset = len(path1_series)  # Offset for the second block in the matrix
for i, series_name in enumerate(path2_series):
  print(f"\nEvaluating {series_name} (Path 2):")
  # Get test data for evaluation (use only first 12 months if test period is longer)
  test_data = series_dict[f"{series_name}_test"][:12]

  # Linear Regression
  print("Linear Regression:")
  get_metrics(reconciled_lr[i + offset], test_data, f"LR {series_name}")
  plot_forecasts(test_data, reconciled_lr[i + offset], f"{series_name} LR Path2", test_dates[:12])

  # Exponential Smoothing
  print("Exponential Smoothing:")
  get_metrics(reconciled_exp[i + offset], test_data, f"EXP {series_name}")
  plot_forecasts(test_data, reconciled_exp[i + offset], f"{series_name} EXP Path2", test_dates[:12])

  # ARIMA
  print("ARIMA:")
  get_metrics(reconciled_arima[i + offset], test_data, f"ARIMA {series_name}")
  plot_forecasts(test_data, reconciled_arima[i + offset], f"{series_name} ARIMA Path2", test_dates[:12])

generate_and_print_forecast_for_model("LR", path2_series, test_dates, reconciled_lr)
generate_and_print_forecast_for_model("EXP", path2_series, test_dates, reconciled_exp)
generate_and_print_forecast_for_model("ARIMA", path2_series, test_dates, reconciled_arima)

"""## Reconciliation self-implementation: TopDown"""

# Forecast EBIT (top-level series)
ebit_series = series_dict['EBIT']
lr_ebit, exp_ebit, arima_ebit = forecast_with_models(ebit_series, train_dates, n_periods=12)

# Compute proportions
proportions = compute_topdown_proportions(train_df)

# Create G matrix for top-down
G_topdown = create_topdown_G_matrix(proportions)

# Disaggregate each top-level forecast
bottom_lr_forecast = disaggregate_forecast(lr_ebit, G_topdown) # shape: (n_bottom_series, n_periods)
bottom_exp_forecast = disaggregate_forecast(exp_ebit, G_topdown)
bottom_arima_forecast = disaggregate_forecast(arima_ebit, G_topdown)

# Optional: Convert to DataFrame for easier viewing
bottom_labels = ['NetSales', '-VariableCosts', '-FixCosts', 'EBITDA', '-DepreciationAmortization']
forecast_dates = test_dates[:12].values

df_topdown_lr = pd.DataFrame(bottom_lr_forecast.T, columns=bottom_labels, index=forecast_dates)
df_topdown_exp = pd.DataFrame(bottom_exp_forecast.T, columns=bottom_labels, index=forecast_dates)
df_topdown_arima = pd.DataFrame(bottom_arima_forecast.T, columns=bottom_labels, index=forecast_dates)

# Path 1 Evaluation
print("\nPath 1 Results:")
for i, series_name in enumerate(path1_series):
    print(f"\nEvaluating {series_name} (Path 1):")
    test_data = series_dict[f"{series_name}_test"][:12]  # Use the first 12 months of the test data

    # Linear Regression
    print("Linear Regression:")
    get_metrics(reconciled_lr[i], test_data, f"LR {series_name}")
    plot_forecasts(test_data, reconciled_lr[i], f"{series_name} LR Path1", test_dates[:12])

    # Exponential Smoothing
    print("Exponential Smoothing:")
    get_metrics(reconciled_exp[i], test_data, f"EXP {series_name}")
    plot_forecasts(test_data, reconciled_exp[i], f"{series_name} EXP Path1", test_dates[:12])

    # ARIMA
    print("ARIMA:")
    get_metrics(reconciled_arima[i], test_data, f"ARIMA {series_name}")
    plot_forecasts(test_data, reconciled_arima[i], f"{series_name} ARIMA Path1", test_dates[:12])

# Generate forecast summaries for Path 1
generate_and_print_forecast_for_model("LR", path1_series, test_dates, reconciled_lr)
generate_and_print_forecast_for_model("EXP", path1_series, test_dates, reconciled_exp)
generate_and_print_forecast_for_model("ARIMA", path1_series, test_dates, reconciled_arima)

# Path 2 Evaluation
print("\nPath 2 Results:")
for i, series_name in enumerate(path2_series):
    print(f"\nEvaluating {series_name} (Path 2):")
    test_data = series_dict[f"{series_name}_test"][:12]  # Use the first 12 months of the test data

    # Linear Regression
    print("Linear Regression:")
    get_metrics(reconciled_lr[i], test_data, f"LR {series_name}")
    plot_forecasts(test_data, reconciled_lr[i], f"{series_name} LR Path2", test_dates[:12])

    # Exponential Smoothing
    print("Exponential Smoothing:")
    get_metrics(reconciled_exp[i], test_data, f"EXP {series_name}")
    plot_forecasts(test_data, reconciled_exp[i], f"{series_name} EXP Path2", test_dates[:12])

    # ARIMA
    print("ARIMA:")
    get_metrics(reconciled_arima[i], test_data, f"ARIMA {series_name}")
    plot_forecasts(test_data, reconciled_arima[i], f"{series_name} ARIMA Path2", test_dates[:12])

# Generate forecast summaries for Path 2
generate_and_print_forecast_for_model("LR", path2_series, test_dates, reconciled_lr)
generate_and_print_forecast_for_model("EXP", path2_series, test_dates, reconciled_exp)
generate_and_print_forecast_for_model("ARIMA", path2_series, test_dates, reconciled_arima)

"""## Time Series Decomposition

https://medium.com/@heyamit10/time-series-decomposition-in-python-049b72a00ba0

https://www.kaggle.com/code/bextuychiev/advanced-time-series-analysis-decomposition
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

"""Level: The average value in the series.

Trend: The increasing or decreasing value in the series.

Seasonality: The repeating short-term cycle in the series.

Noise (Residual) : The random variation in the series. What's left after you strip away the trend and seasonality — the random noise.

"""

from statsmodels.tsa.seasonal import seasonal_decompose
# Decompose the time series (assume an additive model)
result = seasonal_decompose(df['EBIT'], model='additive', period=12)
# Plot the decomposition
result.plot()
plt.show()

from statsmodels.tsa.filters.hp_filter import hpfilter
# Apply the Hodrick-Prescott filter
trend, cycle = hpfilter(df['EBIT'], lamb=1600)  # 'lamb' is the smoothing parameter
# Plot the trend and cycle components
plt.plot(trend, label='Trend')
plt.plot(cycle, label='Cycle')
plt.legend()
plt.show()

import statsmodels.api as sm
# Apply LOESS smoothing
loess = sm.nonparametric.lowess(df['EBIT'], df.index, frac=0.2)
# Plot the smoothed line
plt.plot(df.index, loess[:, 1], label='LOESS Smoothed')
plt.show()

from statsmodels.tsa.seasonal import STL

df.index = pd.to_datetime(df.index)
# Apply STL decomposition
stl = STL(df['EBIT'], seasonal=13, period=6)  # Ensure period is set
result = stl.fit()
result.plot()



# Load the data and adjust negative values as needed
df = pd.read_excel(file_path)
df["-DepreciationAmortization"] = -df["DepreciationAmortization"]
df["-FixCosts"] = -df["FixCosts"]
df["-VariableCosts"] = -df["VariableCosts"]

# Create a dictionary of TimeSeries for each variable
series_dict = {
    "EBIT": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBIT"]),
    "EBITDA": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["EBITDA"]),
    "-DepreciationAmortization": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-DepreciationAmortization"]),
    "ContributionMargin1": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["ContributionMargin1"]),
    "-FixCosts": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-FixCosts"]),
    "NetSales": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["NetSales"]),
    "-VariableCosts": TimeSeries.from_dataframe(df, time_col="Date", value_cols=["-VariableCosts"]),
}

# Train models separately for each series and forecast
forecasts_dict = {}
for key, ts in series_dict.items():
    # Split each series into training and validation segments
    train, val = ts[:-24], ts[-24:-12]

    # Choose the model based on the key
    if key in ["EBIT", "EBITDA"]:
        model = AutoARIMA(lags=12)
    else:
        model = LinearRegressionModel(lags=12)

    # Fit the model, explicitly passing future_covariates (None if not used)
    model.fit(train, future_covariates=None)

    # Predict for the length of the validation period, also passing future_covariates=None
    forecast = model.predict(n=len(val), future_covariates=None)
    forecasts_dict[key] = forecast

# Combine all forecasts into a multivariate TimeSeries (each column corresponds to one variable)
forecasts = concatenate(list(forecasts_dict.values()), axis=1)

components_to_show = ["EBIT","EBITDA", "DepreciationAmortization", "ContributionMargin1", "FixCosts", "NetSales", "VariableCosts"]
plt.figure(figsize=(10, 8))

# Create a multivariate TimeSeries using all desired columns
series = TimeSeries.from_dataframe(df, time_col='Date', value_cols=components_to_show)
pred[components_to_show].plot(lw=1, color='red')
plt.legend(["Series", "Prediction"], loc="upper left")
plt.show()

model = LinearRegressionModel(lags=12)
model.fit(train)
pred = model.predict(n=len(val))

# Define hierarchical levels
level_1 = ["EBIT"]  # Top-level
level_2 = ["ContributionMargin1", "-FixCosts"]
level_3 = ["EBITDA", "-DepreciationAmortization"]
level_4 = ["NetSales", "-VariableCosts","-FixCosts"]
# Define a function to measure MAE
def measure_mae(pred, val):
    def print_mae_on_subset(subset, name):
        mae_value = mae(pred[subset], val[subset])
        print(f"mean MAE on {name}: {mae_value:.2f}")

    # Measure MAE at each hierarchical level
    print_mae_on_subset(level_1, "Level 1 (Top-Level: EBIT)")
    print_mae_on_subset(level_2, "Level 2 Components)")
    print_mae_on_subset(level_3, "Level 3 Components)")
    print_mae_on_subset(level_3, "Level 4 Components)")

# Call the function
measure_mae(pred, val)


get_metrics(forecast_test, test_series, 'LR')

def plot_forecast_sums(pred_series):
    plt.figure(figsize=(10, 5))

    # Define levels in the hierarchy
    level_1 = ["EBIT"]  # Top-level
    level_2 = ["ContributionMargin1", "-FixCosts"]
    level_3 = ["EBITDA", "-DepreciationAmortization"]
    level_4 = ["NetSales", "-VariableCosts","-FixCosts"]

    # Plot the top-level component (EBIT)
    pred_series[level_1].sum(axis=1).plot(
        label="Level 1 (Total EBIT)", lw=6, alpha=0.3, color="grey"
    )
    pred_series[level_2].sum(axis=1).plot(label="Sum of Level 2 components",alpha = 0.7)
    pred_series[level_3].sum(axis=1).plot(label="Sum of Level 3 components",alpha = 0.7)
    pred_series[level_4].sum(axis=1).plot(label="Sum of Level 4 components",alpha = 0.7)


    # Add legend and customize appearance
    legend = plt.legend(loc="best", frameon=1)
    frame = legend.get_frame()
    frame.set_facecolor("white")

    plt.title("Forecast Sums at Different Hierarchical Levels")
    plt.show()


# Call the function with your predicted series
plot_forecast_sums(pred)

# Define hierarchical levels
level_1 = ["EBIT"]  # Top-level
level_2 = ["ContributionMargin1", "-FixCosts"]
level_3 = ["EBITDA", "-DepreciationAmortization"]
level_4 = ["NetSales", "-VariableCosts","-FixCosts"]
# Define a function to measure MAE
def measure_mae(pred, val):
    def print_mae_on_subset(subset, name):
        mae_value = mae(pred[subset], val[subset])
        print(f"mean MAE on {name}: {mae_value:.2f}")

    # Measure MAE at each hierarchical level
    print_mae_on_subset(level_1, "Level 1 (Top-Level: EBIT)")
    print_mae_on_subset(level_2, "Level 2 Components)")
    print_mae_on_subset(level_3, "Level 3 Components)")
    print_mae_on_subset(level_3, "Level 4 Components)")

# Call the function
measure_mae(pred, val)

reconciliator = MinTReconciliator(method="wls_val")
reconciliator.fit(train)
reconcilied_preds = reconciliator.transform(pred)
measure_mae(reconcilied_preds, val)
plot_forecast_sums(reconcilied_preds)

reconciliator1 = BottomUpReconciliator()
reconcilied1_preds = reconciliator1.transform(pred)
measure_mae(reconcilied1_preds, val)
plot_forecast_sums(reconcilied1_preds)

reconciliator2 = TopDownReconciliator()
reconciliator2.fit(train)
reconcilied2_preds = reconciliator2.transform(pred)
measure_mae(reconcilied2_preds, val)
plot_forecast_sums(reconcilied2_preds)

def measure_mae(pred, val):
    def print_mae_on_subset(subset, name):
        mae_value = mae(pred[subset], val[subset])
        print(f"mean MAE on {name}: {mae_value:.2f}")

    # Measure MAE at each hierarchical level
    print_mae_on_subset(level_1, "Level 1 (Top-Level: EBIT)")
    print_mae_on_subset(level_2, "Level 2 Components)")
    print_mae_on_subset(level_3, "Level 3 Components)")
    print_mae_on_subset(level_3, "Level 4 Components)")

from sklearn.metrics import mean_absolute_error as mae, mean_squared_error, r2_score
import numpy as np

# Define function to measure multiple metrics
def measure_metrics(pred, val):
    def print_metrics_on_subset(subset, name):
        mae_value = mae(pred[subset], val[subset])
        rmse_value = np.sqrt(mean_squared_error(pred[subset], val[subset]))
        r2_value = r2_score(val[subset], pred[subset])
        mape_value = np.mean(np.abs((val[subset] - pred[subset]) / val[subset])) * 100

        print(f"Metrics for {name}:")
        print(f" - MAE: {mae_value:.2f}")
        print(f" - RMSE: {rmse_value:.2f}")
        print(f" - R² Score: {r2_value:.2f}")
        print(f" - MAPE: {mape_value:.2f}%\n")

    # Measure metrics at each hierarchical level
    print_metrics_on_subset(level_1, "Level 1 (Top-Level: EBIT)")
    print_metrics_on_subset(level_2, "Level 2 Components")
    print_metrics_on_subset(level_3, "Level 3 Components")
    print_metrics_on_subset(level_4, "Level 4 Components")


measure_metrics(reconcilied1_preds, val)
plot_forecast_sums(reconcilied1_preds)

from sklearn.metrics import mean_absolute_error as mae, mean_squared_error, r2_score
import numpy as np
import pandas as pd

# Function to compute all metrics safely
def measure_metrics(pred, val):
    def safe_convert(timeseries_obj):
        """Ensure the input is a NumPy array before indexing."""
        if hasattr(timeseries_obj, "values"):  # Check if it's a TimeSeries object
            return timeseries_obj.values  # Convert TimeSeries to NumPy array
        elif isinstance(timeseries_obj, pd.Series):
            return timeseries_obj.to_numpy()  # Convert Pandas Series to NumPy
        elif isinstance(timeseries_obj, np.ndarray):
            return timeseries_obj  # Already a NumPy array
        else:
            raise TypeError(f"Unsupported type: {type(timeseries_obj)}")

    # Fully convert `pred` and `val` **before** indexing
    pred_array = safe_convert(pred)
    val_array = safe_convert(val)

    def print_metrics_on_subset(subset_indices, name):
        try:
            print(f"Accessing subset: {name}, Type: {type(subset_indices)}")

            # Extract subset using index list
            pred_subset = pred_array[:, subset_indices, 0]  # Remove last dimension (if shape is (12,7,1))
            val_subset = val_array[:, subset_indices, 0]

            print(f"Subset shape: Pred {pred_subset.shape}, Val {val_subset.shape}")

            if pred_subset.shape != val_subset.shape:
                print(f"Warning: Shape mismatch for {name} (Pred: {pred_subset.shape}, Val: {val_subset.shape})")
                return

            mae_value = mae(pred_subset, val_subset)
            rmse_value = np.sqrt(mean_squared_error(pred_subset, val_subset))
            r2_value = r2_score(val_subset, pred_subset)
            mape_value = np.mean(np.abs((val_subset - pred_subset) / (val_subset + 1e-10))) * 100  # Avoid div by zero

            print(f"Metrics for {name}:")
            print(f" - MAE: {mae_value:.2f}")
            print(f" - RMSE: {rmse_value:.2f}")
            print(f" - R² Score: {r2_value:.2f}")
            print(f" - MAPE: {mape_value:.2f}%\n")

        except Exception as e:
            print(f"Error computing metrics for {name}: {e}")

    # Measure metrics at each hierarchical level
    print_metrics_on_subset(level_1, "Level 1 (Top-Level: EBIT)")
    print_metrics_on_subset(level_2, "Level 2 Components")
    print_metrics_on_subset(level_3, "Level 3 Components")
    print_metrics_on_subset(level_4, "Level 4 Components")

# Run the function
model = LinearRegressionModel(lags=12)
model.fit(train)
pred = model.predict(n=len(val))

reconciliator1 = BottomUpReconciliator()
reconcilied1_preds = reconciliator1.transform(pred)

measure_metrics(reconcilied1_preds, val)
plot_forecast_sums(reconcilied1_preds)

reconcilied1_preds

# Convert TimeSeries to NumPy arrays and remove the last dimension
pred_array = reconcilied1_preds.values.squeeze()  # Shape: (12, 7)
val_array = val.values.squeeze()                  # Shape: (12, 7)

# Verify the shapes
print(f"Shape of Predictions Array: {pred_array.shape}")
print(f"Shape of Actual Values Array: {val_array.shape}")

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Step 1: Convert TimeSeries to NumPy arrays and remove the singleton 'sample' dimension
pred_array = reconcilied1_preds.values.squeeze()  # Shape: (12, 7)
val_array = val.values.squeeze()                # Shape: (12, 7)

# Verify the shapes
print(f"Shape of Predictions Array: {pred_array.shape}")
print(f"Shape of Actual Values Array: {val_array.shape}")

# Step 2: Retrieve component names
components = reconcilied1_preds.components
print("Components:", components)

# Step 3: Define hierarchical levels manually based on your hierarchy
levels = [
    (['EBIT'], "Level 1 (Top-Level: EBIT)"),
    (['EBITDA', '-DepreciationAmortization', '-FixCosts'], "Level 2 Components"),
    (['ContributionMargin1', 'NetSales'], "Level 3 Components"),
    (['-VariableCosts'], "Level 4 Components")
]

# Convert component names to indices using .get_loc()
levels_with_indices = []
for subset_names, level_name in levels:
    subset_indices = []
    for comp in subset_names:
        if comp in components:
            subset_indices.append(components.get_loc(comp))
        else:
            print(f"Warning: Component '{comp}' not found for {level_name}.")
    if subset_indices:
        levels_with_indices.append((subset_indices, level_name))

# Step 4: Define the measure_metrics function
def measure_metrics(pred_array, val_array, components, levels):
    """
    Computes and prints MAE, MAPE, R², and RMSE for each hierarchical level.

    Parameters:
    - pred_array (np.ndarray): Predicted values, shape (time_steps, components)
    - val_array (np.ndarray): Actual values, shape (time_steps, components)
    - components (pd.Index): List of component names
    - levels (list of tuples): Each tuple contains a list of component indices and a level name
    """
    for subset_indices, level_name in levels:
        try:
            # Extract subsets
            pred_subset = pred_array[:, subset_indices]
            val_subset = val_array[:, subset_indices]

            # Compute Metrics
            mae_val = mean_absolute_error(val_subset, pred_subset)
            rmse_val = np.sqrt(mean_squared_error(val_subset, pred_subset))
            r2_val = r2_score(val_subset, pred_subset, multioutput='uniform_average')
            mape_val = np.mean(np.abs((val_subset - pred_subset) / (val_subset + 1e-10))) * 100  # Avoid division by zero

            # Retrieve component names
            subset_names = [components[i] for i in subset_indices]

            # Print Metrics
            print(f"Metrics for {level_name} ({', '.join(subset_names)}):")
            print(f" - MAE: {mae_val:.2f}")
            print(f" - RMSE: {rmse_val:.2f}")
            print(f" - R² Score: {r2_val:.2f}")
            print(f" - MAPE: {mape_val:.2f}%\n")

        except Exception as e:
            print(f"Error computing metrics for {level_name}: {e}")

# Step 5: Call the measure_metrics function
measure_metrics(pred_array, val_array, components, levels_with_indices)

"""#### Backtesting"""

# Load the data from an Excel file
file_path = '/content/SampleHierForecastingBASF_share.xlsx'  # Path to your Excel file
df = pd.read_excel(file_path)  # Specify the sheet name or index if needed

# Display the first few rows to verify
print("Data Preview:\n", df.head())

def plot_backtests(series, forecast_horizon=6):
    """
    Function to plot backtests and forecast results for different models.

    Parameters:
    - series: TimeSeries data
    - forecast_horizon: The forecast horizon in months (1, 3, 6, 12)
    """
    plt.figure(figsize=(10, 6))

    # Define models to use for forecasting
    models = [ExponentialSmoothing(), Prophet()]

    # Perform backtests for each model
    backtests = [model.historical_forecasts(series['EBIT'], start=0.5, forecast_horizon=forecast_horizon) for model in models]

    # Plot actual data
    series['EBIT'].plot(label='Actual Data')

    # Loop over each model to plot the backtest and print metrics
    for i, m in enumerate(models):
        # Plot the backtest forecast (only for forecast horizon)
        backtest = backtests[i]
        backtest.plot(lw=1, label=f'{m} Forecast')

        # Get the actual values from the series for the forecast horizon
        actual_values = series['EBIT'][-forecast_horizon:]

        # Print metrics using the get_metrics function
        get_metrics(backtest, actual_values, f'{m}')

    plt.title(f'Backtests with {forecast_horizon}-months forecast horizon')
    plt.legend()
    plt.show()

# Example usage:
plot_backtests(series, forecast_horizon=6)

plot_backtests(series, forecast_horizon=3)

# Function to compute RMSE
def rmse(y_true, y_pred):
    return np.sqrt(mse(y_true, y_pred))

# Plot the backtests and forecast results
plt.figure(figsize=(10, 6))

models = [ExponentialSmoothing(), Prophet()]
backtests = [model.historical_forecasts(series['EBIT'], start=0.5, forecast_horizon=6) for model in models]

# Plot the actual data
series['EBIT'].plot(label='Actual Data')

# Loop over each model to plot the backtest and print the metrics
for i, m in enumerate(models):
    # Get the backtest forecast
    forecast = backtests[i]

    # Compute the metrics
    err_mape = np.mean(np.abs((forecast - series['EBIT'][-6:]) / series['EBIT'][-6:])) * 100  # MAPE
    err_mae = mae(series['EBIT'][-6:], forecast)  # MAE
    err_r2 = r2_score(series['EBIT'][-6:], forecast)  # R²
    err_rmse = rmse(series['EBIT'][-6:], forecast)  # RMSE

    # Print the metrics separately
    print(f"Model {m}:")
    print(f"MAPE: {err_mape:.2f}%")
    print(f"MAE: {err_mae:.2f}")
    print(f"RMSE: {err_rmse:.2f}")
    print(f"R²: {err_r2:.2f}")
    print("-" * 50)

    # Plot the backtest forecast
    forecast.plot(lw=1, label=f'{m} Forecast')

# Set plot title and legend
plt.title('Backtests with 6-months forecast horizon')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
models = [ExponentialSmoothing(), Prophet()]
backtests = [model.historical_forecasts(series['EBIT'], start=.5, forecast_horizon=6) for model in models]

series['EBIT'].plot(label='data')
for i, m in enumerate(models):
    err_mape = mape(backtests[i], series['EBIT'])
    err_mae = mae(backtests[i], series['EBIT'])
    err_r2 = r2_score(backtests[i], series['EBIT'])
    err_rmse = rmse(backtests[i], series['EBIT'])

    backtests[i].plot(
        lw=1,
        label='{}, MAPE={:.2f}%, MAE={:.2f}, R²={:.2f}, RMSE={:.2f}'.format(
            m, err_mape, err_mae, err_r2, err_rmse
        )
    )

plt.title('Backtests with 6-months forecast horizon')
plt.legend()
plt.show()

from darts.metrics import mape, mae, r2_score, rmse
import matplotlib.pyplot as plt
from darts.models import Prophet


plt.figure(figsize=(10, 6))
models = [ExponentialSmoothing(), Prophet()]
backtests = [model.historical_forecasts(series['EBIT'], start=.5, forecast_horizon=12) for model in models]

series['EBIT'].plot(label='data')
for i, m in enumerate(models):
    err_mape = mape(backtests[i], series['EBIT'])
    err_mae = mae(backtests[i], series['EBIT'])
    err_r2 = r2_score(backtests[i], series['EBIT'])
    err_rmse = rmse(backtests[i], series['EBIT'])

    backtests[i].plot(
        lw=1,
        label='{}, MAPE={:.2f}%, MAE={:.2f}, R²={:.2f}, RMSE={:.2f}'.format(
            m, err_mape, err_mae, err_r2, err_rmse
        )
    )

plt.title('Backtests with 12-month forecast horizon')
plt.legend()
plt.show()

# Function to compute RMSE
def rmse(y_true, y_pred):
    return np.sqrt(mse(y_true, y_pred))

# Function to evaluate and plot forecasts for different forecast horizons
def evaluate_models_with_horizons(series, horizons=[1, 3, 6, 12]):
    # Store metrics for each horizon
    all_metrics = []

    plt.figure(figsize=(10, 6))

    models = [ExponentialSmoothing(), Prophet()]

    for horizon in horizons:
        backtests = [model.historical_forecasts(series['EBIT'], start=0.5, forecast_horizon=horizon) for model in models]

        # Plot the actual data
        series['EBIT'].plot(label='Actual Data')

        # Loop over each model to plot the backtest and print the metrics
        for i, m in enumerate(models):
            # Get the backtest forecast
            forecast = backtests[i]

            # Compute the metrics for each horizon
            err_mape = np.mean(np.abs((forecast - series['EBIT'][-horizon:]) / series['EBIT'][-horizon:])) * 100  # MAPE
            err_mae = mae(series['EBIT'][-horizon:], forecast)  # MAE
            err_r2 = r2_score(series['EBIT'][-horizon:], forecast)  # R²
            err_rmse = rmse(series['EBIT'][-horizon:], forecast)  # RMSE

            # Print the metrics for the current horizon
            print(f"Model {m}, Horizon {horizon} months:")
            print(f"MAPE: {err_mape:.2f}%")
            print(f"MAE: {err_mae:.2f}")
            print(f"RMSE: {err_rmse:.2f}")
            print(f"R²: {err_r2:.2f}")
            print("-" * 50)

            # Plot the backtest forecast for the current horizon
            forecast.plot(lw=1, label=f'{m} Horizon={horizon} months')

        # Set plot title and legend for each horizon
        plt.title(f'Backtests with {horizon}-months forecast horizon')
        plt.legend()
        plt.show()

        # Store metrics in a list for external table
        all_metrics.append({
            'Horizon': horizon,
            'MAPE': err_mape,
            'MAE': err_mae,
            'RMSE': err_rmse,
            'R²': err_r2
        })

    # Convert the metrics list to a DataFrame for an external table
    import pandas as pd
    metrics_df = pd.DataFrame(all_metrics)

    # Display the performance table
    print("\nPerformance Metrics Table:")
    print(metrics_df)


# Call the function with different forecast horizons
evaluate_models_with_horizons(series, horizons=[1, 3, 6, 12])

from darts.metrics import mape, mae, r2_score, rmse
import matplotlib.pyplot as plt
from darts.models import Prophet


plt.figure(figsize=(10, 6))
models = [ExponentialSmoothing(), Prophet()]
backtests = [model.historical_forecasts(series['EBIT'], start=.5, forecast_horizon=1) for model in models]

series['EBIT'].plot(label='data')
for i, m in enumerate(models):
    err_mape = mape(backtests[i], series['EBIT'])
    err_mae = mae(backtests[i], series['EBIT'])
    err_r2 = r2_score(backtests[i], series['EBIT'])
    err_rmse = rmse(backtests[i], series['EBIT'])

    backtests[i].plot(
        lw=1,
        label='{}, MAPE={:.2f}%, MAE={:.2f}, R²={:.2f}, RMSE={:.2f}'.format(
            m, err_mape, err_mae, err_r2, err_rmse
        )
    )

plt.title('Backtests with 1-month forecast horizon')
plt.legend()
plt.show()

from darts.metrics import mape, mae, r2_score, rmse
import matplotlib.pyplot as plt
from darts.models import Prophet


plt.figure(figsize=(10, 6))
models = [ExponentialSmoothing(), Prophet()]
backtests = [model.historical_forecasts(series['EBIT'], start=.5, forecast_horizon=3) for model in models]

series['EBIT'].plot(label='data')
for i, m in enumerate(models):
    err_mape = mape(backtests[i], series['EBIT'])
    err_mae = mae(backtests[i], series['EBIT'])
    err_r2 = r2_score(backtests[i], series['EBIT'])
    err_rmse = rmse(backtests[i], series['EBIT'])

    backtests[i].plot(
        lw=1,
        label='{}, MAPE={:.2f}%, MAE={:.2f}, R²={:.2f}, RMSE={:.2f}'.format(
            m, err_mape, err_mae, err_r2, err_rmse
        )
    )

plt.title('Backtests with 3-months forecast horizon')
plt.legend()
plt.show()

from darts.metrics import mape

plt.figure(figsize=(10, 6))

series['EBIT'].plot(label='data')
for i, m in enumerate(models):
    err = mape(backtests[i], series['EBIT'])
    backtests[i].plot(lw=1, label='{}, MAPE={:.2f}%'.format(m, err))

plt.title('Backtests with 3-months forecast horizon')
plt.legend()