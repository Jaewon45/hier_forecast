{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c945fd69",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#may need to install darts depending on env\n",
    "!python -m pip install darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from darts.models import (Prophet, LinearRegressionModel, ARIMA,  ExponentialSmoothing, XGBModel,  NBEATSModel, GlobalNaiveAggregate, NaiveDrift)\n",
    "from darts.dataprocessing.transformers import MinTReconciliator, BottomUpReconciliator, TopDownReconciliator\n",
    "from utils import (get_winners,get_best_per_series, load_data, apply_hierarchy, compare_models_multivariate, compare_models_reconciliated, compare_models_univariate)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# changing hvar here changes which hierarchy we're using. A subset of the data or the full data? Options are \n",
    "# 'var0': the whole hierarchy\n",
    "# 'v1': path 1\n",
    "# 'var2': path 2\n",
    "hvar = 'var0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357806bc",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca70821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(file_path='data/SampleHierForecastingBASF_share.xlsx')\n",
    "# changing hvar here changes which hierarchy we're using. A subset of the data or the full data? Options are ['v0','v1','v2'].\n",
    "series, target, covariates, hierarchy = apply_hierarchy(df, hvar=hvar)\n",
    "train, val = target[:-24], target[-24:-12]\n",
    "#past_cov = covariates[:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup for reconciliation\n",
    "hierarchical_train = series[:-24]\n",
    "hierarchical_val = series[-24:-12]\n",
    "\n",
    "reconciliator0 = MinTReconciliator(method=\"ols\")\n",
    "reconciliator0.fit(series[:-24])\n",
    "reconciliator1 = TopDownReconciliator()\n",
    "reconciliator1.fit(series[:-24])\n",
    "reconciliator2 = BottomUpReconciliator()\n",
    "\n",
    "reconciliators = [reconciliator0,reconciliator1,reconciliator2,]\n",
    "names = ['MiNT','Top Down', 'Bottom Up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_models_to_test = [\n",
    "    ARIMA(q=1),\n",
    "    ExponentialSmoothing(),\n",
    "    Prophet(),\n",
    "    NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),\n",
    "    LinearRegressionModel(lags=12),\n",
    "    XGBModel(lags=12)\n",
    "    ]\n",
    "multi_models_to_test  = [\n",
    "    #NBEATS params derived from optuna. source code found in hpo.py.\n",
    "    NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),\n",
    "    LinearRegressionModel(lags=12),\n",
    "    XGBModel(lags=12)\n",
    "    ]\n",
    "simple_models_to_test = [\n",
    "    ARIMA(q=1),\n",
    "    ExponentialSmoothing(),\n",
    "    Prophet(),\n",
    "    #NBEATSModel(input_chunk_length=36,output_chunk_length=24, dropout= 0.11891699976631348, n_epochs=27, batch_size=128),\n",
    "    #LinearRegressionModel(lags=12, lags_past_covariates=12),\n",
    "    LinearRegressionModel(lags=12),\n",
    "    #XGBModel(lags=12, lags_past_covariates=12),\n",
    "    #XGBModel(lags=12),\n",
    "    NaiveDrift(),\n",
    "    GlobalNaiveAggregate(input_chunk_length=3, output_chunk_length=3),\n",
    "    GlobalNaiveAggregate(input_chunk_length=1, output_chunk_length=1),\n",
    "    GlobalNaiveAggregate(input_chunk_length=12, output_chunk_length=12),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caba77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi-model approach, find the best model from simple_models for each series.\n",
    "best_per_series = get_best_per_series(hierarchical_train, hierarchical_val,models=simple_models_to_test)\n",
    "best_per_series=best_per_series.with_hierarchy(hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6034005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare univariate baseline models, uncreconciled multivariate models, reconciled multivariate models\n",
    "# includes multi-model in the multivariate models\n",
    "fittedbaselinemodels, univariate_predictions = compare_models_univariate(train, val, uni_models_to_test, past_cov)\n",
    "unreconciled_models, unreconciled_predictions = compare_models_multivariate(hierarchical_train, hierarchical_val, multi_models_to_test)\n",
    "unreconciled_predictions['Multi Model']=best_per_series\n",
    "reconciliatedpredictions = compare_models_reconciliated(data=hierarchical_train, val=val['EBIT'], models=unreconciled_predictions, reconciliators=reconciliators, reconciliator_names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594385a8",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged={**unreconciled_predictions,\n",
    "**reconciliatedpredictions,}\n",
    "\n",
    "merged = {k:v['EBIT'] for k,v in merged.items()}\n",
    "\n",
    "merged = {**univariate_predictions,**merged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca95add",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for model_name, ts in merged.items():\n",
    "    df = ts.to_dataframe().reset_index()  \n",
    "    df.columns = [\"Date\", \"Predictions\"]   \n",
    "    df[\"Name\"] = model_name\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged_df = merged_df[[\"Date\", \"Name\", \"Predictions\"]]\n",
    "merged_df['Date']=pd.to_datetime(merged_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly, summary = get_winners(merged_df,val['EBIT'])\n",
    "summary.to_csv('output/results_'+hvar)\n",
    "merged_df.to_csv('output/predictions_'+hvar)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
